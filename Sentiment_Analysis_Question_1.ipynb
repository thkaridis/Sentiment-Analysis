{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise we have to use the training, development and test examples, in order to create and evaluate a supervised classifier for tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the files (.tsv) inside a file called train and concat them to pandas dataframe called train and also define the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16045 entries, 0 to 9664\n",
      "Data columns (total 3 columns):\n",
      "id           16045 non-null int64\n",
      "sentiment    16045 non-null object\n",
      "text         16045 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 501.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cols = ['id','sentiment','text']\n",
    "path =r'/home/mscuser/Desktop/language_proc_exercise_2/train/' # use your path\n",
    "allFiles = glob.glob(path + \"/*.tsv\")\n",
    "train = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,header=None, names=cols, sep = '\\t')\n",
    "    list_.append(df)\n",
    "train = pd.concat(list_)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file twitter-2016test-A.tsv inside a file called test and creates pandas dataframe called test                          \n",
    "Read all the files (.tsv) inside a file called dev-test and concat them to pandas dataframe called dev-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3947 entries, 0 to 1946\n",
      "Data columns (total 3 columns):\n",
      "id           3947 non-null int64\n",
      "sentiment    3947 non-null object\n",
      "text         3947 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 123.3+ KB\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"/home/mscuser/Desktop/language_proc_exercise_2/test/twitter-2016test-A.tsv\", sep='\\t', header=None, names=cols)\n",
    "path =r'/home/mscuser/Desktop/language_proc_exercise_2/dev-test' # use your path\n",
    "allFiles = glob.glob(path + \"/*.tsv\")\n",
    "devTest = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,header=None, names=cols, sep = '\\t')\n",
    "    list_.append(df)\n",
    "devTest = pd.concat(list_)\n",
    "devTest.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erase all the instances where the twitter text is 'Not Available' and drop the column which contains the id of the twitter comment for each one of the pandas dataframes created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3083 entries, 1 to 1946\n",
      "Data columns (total 2 columns):\n",
      "sentiment    3083 non-null object\n",
      "text         3083 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 72.3+ KB\n"
     ]
    }
   ],
   "source": [
    "devTest = devTest.drop(['id'], axis=1)\n",
    "devTest = devTest[devTest.text != 'Not Available']\n",
    "len(devTest)\n",
    "devTest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15437 entries, 0 to 20341\n",
      "Data columns (total 2 columns):\n",
      "sentiment    15437 non-null object\n",
      "text         15437 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 361.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test = test.drop(['id'], axis=1)\n",
    "test = test[test.text != 'Not Available']\n",
    "len(test)\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11918 entries, 0 to 9663\n",
      "Data columns (total 2 columns):\n",
      "sentiment    11918 non-null object\n",
      "text         11918 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 279.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train = train.drop(['id'], axis=1)\n",
    "train = train[train.text != 'Not Available']\n",
    "len(train)\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check each one by printing the number of neutral, positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     5143\n",
      "positive    5117\n",
      "negative    1658\n",
      "Name: sentiment, dtype: int64\n",
      "positive    1422\n",
      "neutral     1109\n",
      "negative     552\n",
      "Name: sentiment, dtype: int64\n",
      "neutral     7727\n",
      "positive    5439\n",
      "negative    2271\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print train.sentiment.value_counts()\n",
    "print devTest.sentiment.value_counts()\n",
    "print test.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below transform each one of the phrases to seperated words. This is usefull because we want to recognise the word not as it is a very important word for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is responsible for data cleaning. We use BeautifulSoup to create soup objects and we use regular exceptions to remove punctuations, emoticons and references which begin with @. We have to point out that we decided not to exclude stopwords because there are many words that are really important for the sentiment analysis, such as no, not, nor, against etc. An other choice whould be to exclude these words from erasing but the choice we made was to keep all stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        clean = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = souped\n",
    "    stripped = re.sub(r'|'.join((r'@[A-Za-z0-9_]+', r'https?://[^ ]+')), '', clean)\n",
    "    stripped = re.sub(r'www.[^ ]+', '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x  in WordPunctTokenizer().tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we clean each one of the train, test and devtest. Also, we keep a list of sentiments for each one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train = []\n",
    "for t in train.text:\n",
    "    clean_train.append(cleaner(t))\n",
    "len(clean_train)\n",
    "\n",
    "clean_devTest = []\n",
    "for t in devTest.text:\n",
    "    clean_devTest.append(cleaner(t))\n",
    "\n",
    "clean_test = []\n",
    "for t in test.text:\n",
    "    clean_test.append(cleaner(t))\n",
    "\n",
    "train_sentiment = []\n",
    "for t in train.sentiment:\n",
    "    train_sentiment.append(t)\n",
    "\n",
    "devTest_sentiment = []\n",
    "for t in devTest.sentiment:\n",
    "    devTest_sentiment.append(t)\n",
    "\n",
    "test_sentiment = []\n",
    "for t in test.sentiment:\n",
    "    test_sentiment.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CountVectorizer to create a vector for each one of the tweets in the clean_train, clean_test, clean_devTest dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer()\n",
    "Vtrain = countVectorizer.fit_transform(clean_train)\n",
    "Vtest = countVectorizer.transform(clean_test)\n",
    "VdevTest = countVectorizer.transform(clean_devTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we train different models with the train set and we use the dev set for prediction in order to choose the best algorithm for this dataset. To do so we print the accuracy_score for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519299383717\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(Vtrain, train_sentiment)\n",
    "Prediction = model.predict(VdevTest)\n",
    "print accuracy_score(Prediction, devTest_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.467077521894\n"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier()\n",
    "model2.fit(Vtrain, train_sentiment)\n",
    "Prediction = model2.predict(VdevTest)\n",
    "print accuracy_score(Prediction, devTest_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490431397989\n"
     ]
    }
   ],
   "source": [
    "model3 = LogisticRegression()\n",
    "model3.fit(Vtrain, train_sentiment)\n",
    "Prediction = model3.predict(VdevTest)\n",
    "print accuracy_score(Prediction, devTest_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.359714563737\n"
     ]
    }
   ],
   "source": [
    "model4 = SVC()\n",
    "model4.fit(Vtrain, train_sentiment)\n",
    "Prediction = model4.predict(VdevTest)\n",
    "print accuracy_score(Prediction, devTest_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.450210833604\n"
     ]
    }
   ],
   "source": [
    "model5 = DecisionTreeClassifier()\n",
    "model5.fit(Vtrain, train_sentiment)\n",
    "Prediction = model5.predict(VdevTest)\n",
    "print accuracy_score(Prediction, devTest_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the scores above, we are going to use MultinomialNB. It is important to point out that in general, MultinomialNB works better with countVectorizer according to documentation. In the section above, we use gridSearch in order to find the best parameters for our case. In order to perform that, we use the devTest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'fit_prior': 'True'}\n"
     ]
    }
   ],
   "source": [
    "params = {\"alpha\": [0.01, 0.1, 1.0, 10], \"fit_prior\": ['True', 'False']} \n",
    "grid = GridSearchCV(model, params) \n",
    "grid.fit(VdevTest, (np.array(devTest_sentiment)).ravel()) \n",
    "print grid.best_params_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According the results of gridSearch we are going to use the default values for the parameters alpha and fit_prior. Finally we use our model to make predictions for the test and print the accuracy. We can see that the accuracy has been increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.562155859299\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(alpha = 1.0, fit_prior = 'True')\n",
    "model.fit(Vtrain, train_sentiment)\n",
    "Prediction = model.predict(Vtest)\n",
    "print accuracy_score(Prediction, test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pragraph we are going to explain all the choices made for each one of the steps of the performed analysis. To begin with, we have to clarify that we performed sentiments analysis using supervised machine learning algorithm. Before that we clean our tweets by erasing empty tweets, punctuations, emoticons, and @references. Also we use CountVectorizer to vectorize the tweets. With the procedure above, we reduce the number of features. \n",
    "\n",
    "As for the algorithms, we tried MultinomialNB, LogisticRegression, DecisionTree, SVM, RandomForest using train set to train the model and devtest to make the predictions and we chose MultinomialNB because of the accuracy. Then, we used gridSearch and devTest in order to find the best hyperparameters for our model. Finally we trained again our model with the right hyperparameters and made our prediction using test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
